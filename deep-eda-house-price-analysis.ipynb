{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"b6ce74e3-ea1e-44a2-b805-ca2ef8512c75","_cell_guid":"977234ad-40bb-40e3-b262-0c57ace46b7e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-02-03T15:01:28.092677Z","iopub.execute_input":"2023-02-03T15:01:28.094129Z","iopub.status.idle":"2023-02-03T15:01:28.105539Z","shell.execute_reply.started":"2023-02-03T15:01:28.094064Z","shell.execute_reply":"2023-02-03T15:01:28.104066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport category_encoders as ce\nfrom sklearn.feature_selection import SelectKBest, f_classif\n# Importing DataScience libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm,skew\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression,Ridge,RidgeCV, ElasticNetCV, LassoCV,BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler,LabelEncoder,OrdinalEncoder,power_transform\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nimport sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:28.1191Z","iopub.execute_input":"2023-02-03T15:01:28.119911Z","iopub.status.idle":"2023-02-03T15:01:30.282848Z","shell.execute_reply.started":"2023-02-03T15:01:28.119841Z","shell.execute_reply":"2023-02-03T15:01:30.281441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\", encoding = \"UTF-8\")\ntest = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\", encoding = \"UTF-8\")\ntrain_c=train.copy()\ntest_c=test.copy()","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:30.286967Z","iopub.execute_input":"2023-02-03T15:01:30.287403Z","iopub.status.idle":"2023-02-03T15:01:30.368606Z","shell.execute_reply.started":"2023-02-03T15:01:30.287368Z","shell.execute_reply":"2023-02-03T15:01:30.367635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Exploratory Data Analysis\n\n## Objective :\n- To have a better comprehension of our data\n- DÃ©velop a first modelisation strategy\n\n## 1.1 Basic checklist\n#### First analysis :\n- **variable target** : House price\n- **lines / columns** : \n    * training set : 1460 / 81\n    * test set : 1459 / 80\n\n","metadata":{}},{"cell_type":"code","source":"print(\"Our traning set has the following shape : \"+str(train.shape))\nprint(\"Our test set has the following shape : \"+str(test.shape))","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:30.370371Z","iopub.execute_input":"2023-02-03T15:01:30.370773Z","iopub.status.idle":"2023-02-03T15:01:30.377482Z","shell.execute_reply.started":"2023-02-03T15:01:30.370736Z","shell.execute_reply":"2023-02-03T15:01:30.376212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Type of variables** : \n\n* **qualitatives** : 45 (because two variables are already int but correspond to marks on quality) \n* **quantitatives** : 36**\n\n","metadata":{}},{"cell_type":"code","source":"train.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:30.380402Z","iopub.execute_input":"2023-02-03T15:01:30.38088Z","iopub.status.idle":"2023-02-03T15:01:30.403594Z","shell.execute_reply.started":"2023-02-03T15:01:30.380842Z","shell.execute_reply":"2023-02-03T15:01:30.402098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Description of the variable**","metadata":{}},{"cell_type":"markdown","source":"- **PassengerId** - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n- **HomePlanet** - The planet the passenger departed from, typically their planet of permanent residence.\n- **CryoSleep** - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n- **Cabin** - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n- **Destination** - The planet the passenger will be debarking to.\n- **Age** - The age of the passenger.\n- **VIP** - Whether the passenger has paid for special VIP service during the voyage.\n- **RoomService**, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n- **Name** - The first and last names of the passenger.\n- **Transported** - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.","metadata":{}},{"cell_type":"markdown","source":"## **1.2 Checking the missing values**","metadata":{}},{"cell_type":"code","source":"# Checking for the Missing values\n# Using isnull fuction to count the total null values in each field\ntotal = train.isnull().sum().sort_values(ascending=False) \n# Percent of missing values is estimated by dividing total missing and the original total\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n# Concatenating the Total and Percent fields sing pandas concat fucntion\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# Displays top 20 from our max sorted list\nmissing_data.head(20)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:30.405106Z","iopub.execute_input":"2023-02-03T15:01:30.406386Z","iopub.status.idle":"2023-02-03T15:01:30.455101Z","shell.execute_reply.started":"2023-02-03T15:01:30.406286Z","shell.execute_reply":"2023-02-03T15:01:30.453236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**My first approach here would be :**\n\n* Removing the different columns with more than 50% value because it won't help a lot here to predict our target variable\n* We will also remove the Id column as it has nothing to do with our EDA analysis\n","metadata":{}},{"cell_type":"code","source":"train.drop(\"Id\",axis=1,inplace=True)\ntrain.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:30.458957Z","iopub.execute_input":"2023-02-03T15:01:30.459384Z","iopub.status.idle":"2023-02-03T15:01:30.475415Z","shell.execute_reply.started":"2023-02-03T15:01:30.459345Z","shell.execute_reply":"2023-02-03T15:01:30.474098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.3 Checking ditribution of our target variable and independant variables**","metadata":{}},{"cell_type":"markdown","source":"### Target variable","metadata":{}},{"cell_type":"code","source":"y = train['SalePrice']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=stats.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:30.477182Z","iopub.execute_input":"2023-02-03T15:01:30.477666Z","iopub.status.idle":"2023-02-03T15:01:31.833695Z","shell.execute_reply.started":"2023-02-03T15:01:30.477627Z","shell.execute_reply":"2023-02-03T15:01:31.832503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the SalePrice does not follow normal distribution. It seems that the best fit here is the Johnson distribution and log transformation is also a very good constestant ! ","metadata":{}},{"cell_type":"markdown","source":"So confirmed by the kstest, our p-value is really inferior to 0.05, so we can reasonably reject the H0 hypothesis and SalePrice is not following a normal distribution.","metadata":{}},{"cell_type":"code","source":"stats.kstest(np.array(train[\"SalePrice\"]),'norm',args=(train[\"SalePrice\"].mean(),train[\"SalePrice\"].std()))","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:31.835132Z","iopub.execute_input":"2023-02-03T15:01:31.835723Z","iopub.status.idle":"2023-02-03T15:01:31.848644Z","shell.execute_reply.started":"2023-02-03T15:01:31.835673Z","shell.execute_reply":"2023-02-03T15:01:31.847376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Independant variable","metadata":{}},{"cell_type":"markdown","source":"Regarding our indepedant variable, we can check the our k-test also to verify that the other variables are not following a normal distribution.","metadata":{}},{"cell_type":"code","source":"selection=train.select_dtypes(exclude=[\"object\"]).columns\n\nfor col in selection:\n    df=train[col].dropna(axis=0)\n    check_var=np.array(df)\n    res=stats.kstest(check_var,'norm',args=(df.mean(),df.std())).pvalue\n    if res>=0.05:\n        print(\"This column seems to be normally distirbuted : \"+col+\" (p_value=)\"+res)\nprint(\"It's finish\")    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:31.85241Z","iopub.execute_input":"2023-02-03T15:01:31.852815Z","iopub.status.idle":"2023-02-03T15:01:31.939506Z","shell.execute_reply.started":"2023-02-03T15:01:31.85278Z","shell.execute_reply":"2023-02-03T15:01:31.93806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(8,5,figsize=(30,40))\ni=0\n#matplotlib.rcParams['figure.autolayout'] = False\nfor col in train.select_dtypes(exclude=['object']):\n    #plt.figure() remove the line to avoid the text under the plot !\n    #plt.rcParams[\"figure.figsize\"] = (30,40)\n    sns.distplot(train[col],ax=ax[i//5,i%5])\n    i=i+1\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:31.942721Z","iopub.execute_input":"2023-02-03T15:01:31.943168Z","iopub.status.idle":"2023-02-03T15:01:40.253313Z","shell.execute_reply.started":"2023-02-03T15:01:31.943098Z","shell.execute_reply":"2023-02-03T15:01:40.251643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see here, we have also some good client for the log normal distribution. Lot Area, BsmtUnSF, TotalBsmtSF,1stFlrSF,GrLivArea. So we could apply a lognormal transformation after on those different variables.","metadata":{}},{"cell_type":"markdown","source":"## **1.4 The relationship between the target variable and the independant variables**","metadata":{}},{"cell_type":"markdown","source":"So here we will use the mutual information tool as it represents many different advantages. Indeed, Mutual information (MI) is a non-negative value that measures the mutual dependence between two random variables. The mutual information measures the amount of information we can know from one variable by observing the values of the second variable.\n\nThe mutual information is a good alternative to Pearsonâs correlation coefficient, because it is able to measure any type of relationship between variables, not just linear associations. And also, it is suitable for both continuous and discrete variables, unlike Pearsonâs correlation coefficient.\n\nSo first, here we will check this info between the target variables and the independant variables. So we will start our analysis with this.","metadata":{}},{"cell_type":"code","source":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:40.255416Z","iopub.execute_input":"2023-02-03T15:01:40.25588Z","iopub.status.idle":"2023-02-03T15:01:40.26599Z","shell.execute_reply.started":"2023-02-03T15:01:40.25584Z","shell.execute_reply":"2023-02-03T15:01:40.264813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_info=[]\nfor col in train.columns:\n    t=train[[col,\"SalePrice\"]].dropna(axis=0)\n    if col!=\"SalePrice\":\n        mi_score=make_mi_scores(t,t[\"SalePrice\"])\n        mi_info.append((col,mi_score[1]))\n    else:\n        pass\nprint(\"Finish\")","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:01:40.267659Z","iopub.execute_input":"2023-02-03T15:01:40.268048Z","iopub.status.idle":"2023-02-03T15:01:54.353687Z","shell.execute_reply.started":"2023-02-03T15:01:40.268018Z","shell.execute_reply":"2023-02-03T15:01:54.352213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi_scores=pd.DataFrame(mi_info,columns=[\"feature\",\"score_mi\"]).sort_values(by=['score_mi'],ascending=False)\nplt.figure(figsize=(6, 0.25*len(mi_info)))\nsns.barplot(data=mi_scores, y='feature', x='score_mi', orient='h')","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:02:04.646705Z","iopub.execute_input":"2023-02-03T15:02:04.647217Z","iopub.status.idle":"2023-02-03T15:02:06.134806Z","shell.execute_reply.started":"2023-02-03T15:02:04.647175Z","shell.execute_reply":"2023-02-03T15:02:06.13357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, it seems that we have several variables that can be found quite logically :\n\nGrLivArea,GareArea,LotArea, all the variables which are referring to a surfaces.\n\nBut we denote also some more interesting insights with the high contribution of the OverallQuall variable and the neighborhood also.\n","metadata":{}},{"cell_type":"markdown","source":"## Violin plot :\n\nIt could be also interesting to see in the categorical variables the distributions of SalePrices.","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(8,5,figsize=(30,40))\ni=0\nfor col in train.select_dtypes('object'):\n    sns.violinplot(data=train,x=train[col],y=train[\"SalePrice\"],ax=ax[i//5,i%5])\n    i=i+1","metadata":{"execution":{"iopub.status.busy":"2023-02-03T17:04:12.968575Z","iopub.execute_input":"2023-02-03T17:04:12.96914Z","iopub.status.idle":"2023-02-03T17:04:22.220646Z","shell.execute_reply.started":"2023-02-03T17:04:12.969099Z","shell.execute_reply":"2023-02-03T17:04:22.219169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Violin graph are really interesting as it allows us to see the distribution of the variable per category but it's also interesting to the at the same time the range of our numerical variable in this category.\n\nFor example here, if we look at the ExterQual variable, we can see that houses with excellent exterior have higher prices than the others and at the same time the distribution of the sales price is more spread out for ExteriorQual==Ex.","metadata":{}},{"cell_type":"markdown","source":"## Box plot :","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(8,5,figsize=(30,40))\ni=0\nfor col in train.select_dtypes('object'):\n    sns.boxplot(data=train,x=train[col],y=train[\"SalePrice\"],ax=ax[i//5,i%5])\n    i=i+1","metadata":{"execution":{"iopub.status.busy":"2023-02-03T17:05:34.581477Z","iopub.execute_input":"2023-02-03T17:05:34.582392Z","iopub.status.idle":"2023-02-03T17:05:46.027273Z","shell.execute_reply.started":"2023-02-03T17:05:34.582329Z","shell.execute_reply":"2023-02-03T17:05:46.025754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.5 The relationship / interaction the independant variables**","metadata":{}},{"cell_type":"code","source":"pd.plotting.scatter_matrix(train, figsize=(76, 76))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-03T15:54:20.456835Z","iopub.execute_input":"2023-02-03T15:54:20.457394Z","iopub.status.idle":"2023-02-03T15:55:48.518388Z","shell.execute_reply.started":"2023-02-03T15:54:20.457353Z","shell.execute_reply":"2023-02-03T15:55:48.516172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(40,40))\n\ncorr = train.corr(method = 'spearman')\n\nsns.heatmap(corr, annot = True)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-03T16:36:04.319412Z","iopub.execute_input":"2023-02-03T16:36:04.319996Z","iopub.status.idle":"2023-02-03T16:36:10.349496Z","shell.execute_reply.started":"2023-02-03T16:36:04.319948Z","shell.execute_reply":"2023-02-03T16:36:10.348126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So as we can see here, there are plenty of interesting relationship here ! And compared to the pearson correlation, spearman is not only working on linear relationship.\n\nIndeed the fundamental difference between the two correlation coefficients is that the Pearson coefficient works with a linear relationship between the two variables whereas the Spearman Coefficient works with monotonic relationships as well.","metadata":{}},{"cell_type":"markdown","source":"So regarding the interaction, we have for example some logic ones as for example interactions between GarageYblt and Yearbuilt (as it seems logic thath Garage was built at the same time as the garage), we also have the GarageCars and the GarageArea.","metadata":{}},{"cell_type":"markdown","source":"## Categorical features :\n\nIt could be also interesting to use ANOVA test to check the influence of categorical variable on SalePrice. Indeed, the lower is our p-value, the higher the impact of the categorical value is. Indeed in this case, it means, that the mean between the different categories is not the same.","metadata":{"execution":{"iopub.status.busy":"2023-02-03T16:49:03.484372Z","iopub.execute_input":"2023-02-03T16:49:03.484916Z","iopub.status.idle":"2023-02-03T16:49:03.495876Z","shell.execute_reply.started":"2023-02-03T16:49:03.484854Z","shell.execute_reply":"2023-02-03T16:49:03.494329Z"}}},{"cell_type":"code","source":"figure,ax=plt.subplots(1,1,figsize=(50,50))\ndisp=[]\nfor c in train.select_dtypes('object'):\n        samples = []\n        df=train[[c,\"SalePrice\"]].dropna(axis=0)\n        for cls in df[c].unique():\n            s = df[df[c] == cls]['SalePrice'].values\n            samples.append(s)\n        pval = stats.f_oneway(*samples)[1]\n        disp.append((c,np.log(1./pval)))\n\ndisparity=pd.DataFrame(disp,columns=[\"Feature\",\"Disparity\"]).sort_values(by=[\"Disparity\"],ascending=False)\nsns.barplot(data=disparity, x='Feature', y='Disparity')\nax.set_xticklabels(disparity['Feature'].values, rotation=45, ha='right',fontsize=30)\nplt.yticks(fontsize=30)\nplt.xlabel('Feature', fontsize=35)\nplt.ylabel('Disparity', fontsize=35)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-03T18:15:18.005925Z","iopub.execute_input":"2023-02-03T18:15:18.006419Z","iopub.status.idle":"2023-02-03T18:15:19.357419Z","shell.execute_reply.started":"2023-02-03T18:15:18.006374Z","shell.execute_reply":"2023-02-03T18:15:19.355746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I hope you enjoyed reading this notebook ! Do not hesitate to upvote it ! :)**","metadata":{"execution":{"iopub.status.busy":"2023-02-03T18:15:08.504105Z","iopub.execute_input":"2023-02-03T18:15:08.504595Z","iopub.status.idle":"2023-02-03T18:15:08.513947Z","shell.execute_reply.started":"2023-02-03T18:15:08.50455Z","shell.execute_reply":"2023-02-03T18:15:08.512366Z"}}}]}